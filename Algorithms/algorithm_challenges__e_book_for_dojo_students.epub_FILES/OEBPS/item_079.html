<?xml version="1.0" encoding="utf-8" standalone="no"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="application/xhtml+xml; charset=utf-8" /><meta http-equiv="Content-Style-Type" content="text/css" /><meta name="description" content="Computer Science Algorithms Data Structures JavaScript JS" /><meta name="generator" content="Aspose.Words for .NET 13.6.0.0" /><title>Algorithms_Collection</title><link href="item_083.css" type="text/css" rel="stylesheet" /></head><body><div style="clear:both; mso-break-type:section-break; page-break-before:auto"><h2 id="navPoint_121"><span>Numerical Systems</span></h2><p class="Normal1"><span>As </span><span>humans,</span><span> we have been raised from a young age to have a specific attitude toward the number ‘ten’. This is rooted in the fact that as a species we have ten fingers, ten toes. As a result, there are ten numerals in the modern symbolic representation of numbers (0-9). But how would our world look if from the beginning our species had only eight fingers? Or what if we had </span><span style="font-style:italic">sixteen</span><span> fingers? Or what if we, like computers, primarily thought about only “on” and “off” – the equivalent of having only two numerals to choose from? Our writing would look different, although the actual quantities would be the same. </span></p><p class="Normal1"><span>&#xa0;</span></p><p class="Normal1" style="line-height:115%"><span>Numerical systems by nature are simply different ways that symbols can represent a quantity. A certain quantity can be represented by different symbols. Whether we say “</span><span>quarante-deux</span><span>” or “forty-two” or </span><span class="CodeSnippet" style="font-family:Times, serif">0x2A</span><span> or </span><span style="font-style:italic; font-weight:bold">42</span><span>, the amount is the same. So, why can’t we just stay with the </span><span style="font-style:italic">decimal</span><span> (ten-based) system that is ‘native’ and natural to humans? </span></p><p class="Normal1"><span>&#xa0;</span></p><p class="Normal1" style="line-height:115%"><span>Computers don’t think that way, that’s why. Computer architecture from the beginning has evolved from a foundation based on binary digital logic, where signals are either ON or OFF. They live in a world of two numerals, not ten numerals – almost as if they count using two fingers, rather than ten fingers. Each of these 0/1 numerals is called a </span><span style="font-style:italic">bit</span><span>. </span><span>If you can only use 0’s and 1’s then a number like 42 (decimal) becomes </span><span class="CodeSnippet" style="font-family:Times, serif">0b0101010</span><span> (binary). This would make for very long and tedious numbers indeed, except that we have combined these groups of 2, to create number systems that they are closer to the 10-based system that is natural to us. </span></p><p class="Normal1"><br style="page-break-before:always; clear:both" /></p><p class="PageHeader"><span>Chapter 18</span><span> – Bit Arithmetic</span></p><p class="Normal1" style="border-top-color:#000000; border-top-style:solid; border-top-width:0.5pt; padding-top:1pt"><span>&#xa0;</span></p></div></body></html>